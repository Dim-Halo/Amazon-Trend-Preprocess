import pandas as pd
import numpy as np
import os
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
from sentence_transformers import SentenceTransformer
import faiss
import gc
import json
import time

# ================= âš™ï¸ å…¨å±€é…ç½® (Config) =================
CONFIG = {
    'input_folder': './clean_data',          # è¾“å…¥ï¼šæ¸…æ´—åçš„æ•°æ®æ–‡ä»¶å¤¹
    'processed_folder': './processed_data',  # è¾“å‡ºï¼šåŠ äº†æ˜ å°„åˆ—çš„æ•°æ® (for model)
    'check_folder': './mapping_check',       # è¾“å‡ºï¼šäººå·¥æ£€æŸ¥ç”¨ Excel
    'npy_folder': './final_npy',             # è¾“å‡ºï¼šæœ€ç»ˆçŸ©é˜µ
    
    'term_col': 'normalized_term',           # æœç´¢è¯åˆ—å
    'date_col': 'æŠ¥å‘Šæ—¥æœŸ',                   # æ—¥æœŸåˆ—å
    'rank_col': 'æœç´¢é¢‘ç‡æ’å',               # æ’ååˆ—å
    
    'similarity_threshold': 0.75,            # ç›¸ä¼¼åº¦é˜ˆå€¼
    'device': 'cpu'                          # AMD 780M å¼ºåˆ¶ç”¨ CPU
}

# ç¡®ä¿æ–‡ä»¶å¤¹å­˜åœ¨
for folder in [CONFIG['processed_folder'], CONFIG['check_folder'], CONFIG['npy_folder']]:
    if not os.path.exists(folder):
        os.makedirs(folder)

# ================= ğŸ“¦ æ¨¡å— 1: å…¨å±€è¯æ±‡æ”¶é›† =================
def module_1_collect_vocab():
    """
    éå†æ‰€æœ‰æ–‡ä»¶ï¼Œæå–æ‰€æœ‰å”¯ä¸€çš„æœç´¢è¯ã€‚
    """
    print("\nğŸ“¦ [æ¨¡å— 1] å¯åŠ¨: å…¨å±€è¯æ±‡æ”¶é›†...")
    start_time = time.time()
    
    global_vocab = set()
    files = [f for f in os.listdir(CONFIG['input_folder']) if f.endswith('.csv')]
    
    for i, file in enumerate(files):
        path = os.path.join(CONFIG['input_folder'], file)
        try:
            # åªè¯»ä¸€åˆ—ï¼Œé€Ÿåº¦æå¿«
            df = pd.read_csv(path, usecols=[CONFIG['term_col']], encoding='utf-8-sig')
            terms = df[CONFIG['term_col']].dropna().unique().tolist()
            global_vocab.update(terms)
        except Exception as e:
            print(f"   âš ï¸ è·³è¿‡ {file}: {e}")
            
    vocab_list = sorted(list(global_vocab))
    print(f"   âœ… æ”¶é›†å®Œæˆ! å…¨ç½‘å”¯ä¸€è¯æ•°: {len(vocab_list)}")
    print(f"   â±ï¸ è€—æ—¶: {time.time() - start_time:.2f}s")
    return vocab_list

# ================= ğŸ§  æ¨¡å— 2: å‘é‡èšç±»ä¸æ˜ å°„ =================
# ================= ğŸ§  æ¨¡å— 2ï¼ˆç¨³å®šç‰ˆï¼‰: å‘é‡åŒ– + Top-K ç›¸ä¼¼è¯æ˜ å°„ =================
def module_2_build_mapping(vocab_list):
    """
    ã€æ¨èç¨³å®šå®ç°ã€‘
    - ä½¿ç”¨ Top-K æœç´¢æ›¿ä»£ range_search
    - é¿å… FAISS åœ¨ Windows / CPU / ç™¾ä¸‡çº§ä¸‹çš„ C++ abort
    - è¯­ä¹‰æ•ˆæœä¸ range_search åŸºæœ¬ä¸€è‡´
    """
    print("\nğŸ§  [æ¨¡å— 2 - ç¨³å®šç‰ˆ] å¯åŠ¨: å‘é‡åŒ– + Top-K ç›¸ä¼¼æœç´¢ (CPU Safe Mode)")
    start_time = time.time()

    # ---------- 1ï¸âƒ£ åŠ è½½æ¨¡å‹ ----------
    model = SentenceTransformer(
        'all-MiniLM-L6-v2',
        device=CONFIG['device']
    )

    # ---------- 2ï¸âƒ£ å‘é‡åŒ– ----------
    print(f"   âš¡ æ­£åœ¨è®¡ç®— {len(vocab_list)} ä¸ªè¯çš„å‘é‡...")
    embeddings = model.encode(
        vocab_list,
        batch_size=64,
        show_progress_bar=True,
        convert_to_numpy=True
    )

    # å•ä½åŒ–ï¼Œå†…ç§¯ = cosine
    faiss.normalize_L2(embeddings)

    # ---------- 3ï¸âƒ£ æ„å»º FAISS Index ----------
    dim = embeddings.shape[1]
    index = faiss.IndexFlatIP(dim)
    index.add(embeddings)

    # ---------- 4ï¸âƒ£ Top-K æœç´¢ï¼ˆæ ¸å¿ƒå·®å¼‚ç‚¹ï¼‰ ----------
    # âš ï¸ ç»éªŒå€¼ï¼š20ï½30 è¶³å¤Ÿè¦†ç›–æ‰€æœ‰è¿‘ä¹‰è¯
    TOP_K = 20
    SIM_THRESHOLD = CONFIG['similarity_threshold']

    print(f"   ğŸ” æ­£åœ¨æ‰§è¡Œ Top-{TOP_K} ç›¸ä¼¼æœç´¢ (threshold={SIM_THRESHOLD})...")
    D, I = index.search(embeddings, TOP_K)

    # ---------- 5ï¸âƒ£ æ„å»ºæ˜ å°„ ----------
    mapping_dict = {}
    change_log = []

    for i, word in enumerate(vocab_list):
        # å½“å‰è¯çš„ Top-K ç›¸ä¼¼è¯
        sim_scores = D[i]
        neighbor_indices = I[i]

        # è¿‡æ»¤ï¼šåªä¿ç•™ â‰¥ é˜ˆå€¼ çš„
        neighbors = [
            vocab_list[j]
            for j, score in zip(neighbor_indices, sim_scores)
            if score >= SIM_THRESHOLD
        ]

        # ç†è®ºä¸Šè‡³å°‘åŒ…å«è‡ªèº«
        if not neighbors:
            mapping_dict[word] = word
            continue

        # Canonical ç­–ç•¥ï¼šæœ€çŸ­å­—ç¬¦ä¸²
        canonical = min(neighbors, key=len)
        mapping_dict[word] = canonical

        if word != canonical:
            change_log.append({
                'åŸå§‹è¯ (Original)': word,
                'æ˜ å°„å (Mapped)': canonical,
                'åŒç»„è¯æ•°': len(neighbors)
            })

        # å¯é€‰è¿›åº¦æç¤ºï¼ˆä¸å½±å“æ€§èƒ½ï¼‰
        if (i + 1) % 100000 == 0:
            print(f"      å·²å¤„ç† {i + 1:,}/{len(vocab_list):,} ä¸ªè¯", end='\r')

    print(f"\n   âœ… æ˜ å°„å®Œæˆ!")
    print(f"   ğŸ” å‘ç”Ÿæ˜ å°„çš„è¯æ•°: {len(change_log):,}")
    print(f"   â±ï¸ æ€»è€—æ—¶: {time.time() - start_time:.2f}s")

    return mapping_dict, change_log

# ================= ğŸ“ æ¨¡å— 3: è¾“å‡ºéªŒè¯æ–‡ä»¶ (ä½ éœ€æ±‚çš„æ ¸å¿ƒ) =================
def module_3_export_verification(mapping_dict, change_log):
    """
    1. ç”Ÿæˆä¸€ä¸ªæ±‡æ€»çš„ Change Log Excel æ–¹ä¾¿äººå·¥å®¡æŸ¥ã€‚
    2. é‡æ–°å¤„ç† 52 ä¸ª CSVï¼Œå¢åŠ  'mapped_term' åˆ—å¹¶ä¿å­˜ã€‚
    """
    print("\nğŸ“ [æ¨¡å— 3] å¯åŠ¨: ç”ŸæˆéªŒè¯æ–‡ä»¶ä¸å¤„ç†æ•°æ®...")
    
    # --- ä»»åŠ¡ A: å¯¼å‡ºâ€œæ˜ å°„å…³ç³»è¡¨â€ (åªçœ‹å˜åŒ–çš„ï¼Œæ–¹ä¾¿ä½ å¿«é€Ÿæ£€æŸ¥) ---
    if change_log:
        df_log = pd.DataFrame(change_log)
        log_path = os.path.join(CONFIG['check_folder'], 'è¯ä¹‰æ˜ å°„å¯¹ç…§è¡¨_åªçœ‹å˜åŒ–.xlsx')
        # å­˜ä¸º Excelï¼Œæ–¹ä¾¿ä½ ç­›é€‰
        df_log.to_excel(log_path, index=False)
        print(f"   ğŸ‘ï¸ [äººå·¥æ£€æŸ¥] æ˜ å°„å¯¹ç…§è¡¨å·²ä¿å­˜è‡³: {log_path} (è¯·åŠ¡å¿…æ‰“å¼€çœ‹çœ‹!)")
    else:
        print("   âš ï¸ æ²¡æœ‰å‘ç°ä»»ä½•ç›¸ä¼¼è¯åˆå¹¶ï¼Œè¯·æ£€æŸ¥é˜ˆå€¼æ˜¯å¦å¤ªé«˜ã€‚")

    # --- ä»»åŠ¡ B: æ‰¹é‡å¤„ç† 52 ä¸ªæ–‡ä»¶ï¼Œå¢åŠ æ–°åˆ— ---
    print("   ğŸš€ æ­£åœ¨æ‰¹é‡å¤„ç†åŸå§‹æ–‡ä»¶ (å¢åŠ æ˜ å°„åˆ—)...")
    files = [f for f in os.listdir(CONFIG['input_folder']) if f.endswith('.csv')]
    
    processed_file_paths = []
    
    for file in files:
        in_path = os.path.join(CONFIG['input_folder'], file)
        out_path = os.path.join(CONFIG['processed_folder'], f"mapped_{file}")
        
        try:
            # è¯»å–
            df = pd.read_csv(in_path, encoding='utf-8-sig', parse_dates=[CONFIG['date_col']])
            
            # æ ¸å¿ƒæ“ä½œï¼šæ˜ å°„å¹¶æ–°å¢ä¸€åˆ—
            # mapä¸åˆ°çš„è¯ä¿æŒåŸæ · (fillna)
            df['mapped_term'] = df[CONFIG['term_col']].map(mapping_dict).fillna(df[CONFIG['term_col']])
            
            # ä¿å­˜
            df.to_csv(out_path, index=False, encoding='utf-8-sig')
            processed_file_paths.append(out_path)
            
        except Exception as e:
            print(f"   âŒ å¤„ç† {file} å¤±è´¥: {e}")
            
    print(f"   âœ… æ‰€æœ‰æ–‡ä»¶å·²å¤„ç†å¹¶ä¿å­˜è‡³: {CONFIG['processed_folder']}")
    return processed_file_paths

# ================= ğŸ“Š æ¨¡å— 4: ç”Ÿæˆ TimesNet çŸ©é˜µ =================
def module_4_generate_matrix():
    """
    ç›´æ¥æ‰«æ CONFIG['processed_folder'] æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰ CSV æ–‡ä»¶ã€‚
    æ‰§è¡Œ Pivot å’Œ Concatï¼Œç”Ÿæˆæœ€ç»ˆçŸ©é˜µã€‚
    """
    print("\nğŸ“Š [æ¨¡å— 4] å¯åŠ¨: æ‰«ææ–‡ä»¶å¤¹å¹¶ç”ŸæˆçŸ©é˜µ...")
    
    input_folder = CONFIG['processed_folder']
    
    # 1. æ£€æŸ¥æ–‡ä»¶å¤¹æ˜¯å¦å­˜åœ¨
    if not os.path.exists(input_folder):
        print(f"âŒ é”™è¯¯: è¾“å…¥æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {input_folder}")
        print("   è¯·å…ˆè¿è¡Œæ¨¡å— 3 ç”Ÿæˆæ•°æ®ï¼Œæˆ–æ£€æŸ¥è·¯å¾„é…ç½®ã€‚")
        return

    # 2. æ‰«ææ‰€æœ‰ CSV æ–‡ä»¶
    files = [f for f in os.listdir(input_folder) if f.endswith('.csv')]
    print(f"   ğŸ“‚ å‘ç° {len(files)} ä¸ª CSV æ–‡ä»¶ã€‚")
    
    if not files:
        print("âŒ é”™è¯¯: æ–‡ä»¶å¤¹æ˜¯ç©ºçš„ï¼Œæ²¡æœ‰ CSV æ–‡ä»¶ã€‚")
        return

    all_dfs = []
    
    # 3. å¾ªç¯å¤„ç†
    print("   ğŸš€ å¼€å§‹è¯»å–æ•°æ®...")
    for i, file in enumerate(files):
        file_path = os.path.join(input_folder, file)
        
        try:
            # è¯»å– (utf-8-sig å…¼å®¹æ€§æœ€ä½³)
            df = pd.read_csv(file_path, parse_dates=[CONFIG['date_col']], encoding='utf-8-sig')
            
            # æ¸…æ´—åˆ—å
            df.columns = df.columns.str.strip().str.replace('\ufeff', '')
            
            # æ’åè½¬æ•°å­— (å»é™¤é€—å·)
            if df[CONFIG['rank_col']].dtype == object:
                df[CONFIG['rank_col']] = df[CONFIG['rank_col']].astype(str).str.replace(',', '')
            df[CONFIG['rank_col']] = pd.to_numeric(df[CONFIG['rank_col']], errors='coerce')
            
            # èšåˆä¸é€è§†
            df_agg = df.groupby(['mapped_term', CONFIG['date_col']])[CONFIG['rank_col']].min().reset_index()
            df_pivot = df_agg.pivot(index='mapped_term', columns=CONFIG['date_col'], values=CONFIG['rank_col'])
            
            if not df_pivot.empty:
                all_dfs.append(df_pivot)
            
            # è¿›åº¦æ¡
            if (i + 1) % 10 == 0:
                print(f"      å·²å¤„ç† {i + 1}/{len(files)} ä¸ªæ–‡ä»¶...", end='\r')
                
        except Exception as e:
            print(f"   âš ï¸ è·³è¿‡æ–‡ä»¶ {file}: {e}")

    print(f"\n   âœ… æˆåŠŸè¯»å– {len(all_dfs)} ä¸ªæœ‰æ•ˆæ–‡ä»¶çš„æ•°æ®å—ã€‚")

    if not all_dfs:
        print("âŒ é”™è¯¯: æ‰€æœ‰æ–‡ä»¶å¤„ç†åå‡ä¸ºç©ºï¼æ— æ³•ç”ŸæˆçŸ©é˜µã€‚")
        return

    # 4. åˆå¹¶çŸ©é˜µ
    print("   ğŸ§© æ­£åœ¨æ‹¼æ¥å…¨é‡çŸ©é˜µ (Concat)...")
    final_df = pd.concat(all_dfs, axis=1)
    
    # 5. å¤„ç†é‡å¤åˆ— (Transpose Groupby ä¿®å¤ç‰ˆ)
    print("   ğŸ”„ å¤„ç†é‡å¤æ—¥æœŸåˆ— (Transpose Groupby)...")
    final_df = final_df.T.groupby(level=0).min().T
    
    # æŒ‰æ—¶é—´æ’åº
    final_df = final_df.sort_index(axis=1)
    
    # 6. æœ€ç»ˆæ¸…æ´—ä¸ä¿å­˜
    final_df.fillna(2000000, inplace=True) # å¡«å……ç©ºå€¼
    
    # è¿‡æ»¤é€»è¾‘ (ä¿ç•™å†å²æœ€é«˜æ’åå‰ 200ä¸‡ çš„è¯)
    valid_mask = final_df.min(axis=1) < 2000000
    
    final_matrix = np.log10(final_df.loc[valid_mask].values + 1)
    kept_terms = final_df.index[valid_mask]
    kept_dates = final_df.columns
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    if not os.path.exists(CONFIG['npy_folder']):
        os.makedirs(CONFIG['npy_folder'])

    # ä¿å­˜æ–‡ä»¶
    np.save(os.path.join(CONFIG['npy_folder'], 'timesnet_input.npy'), final_matrix)
    pd.Series(kept_terms, name='term').to_csv(os.path.join(CONFIG['npy_folder'], 'terms.csv'), index=False, encoding='utf-8-sig')
    pd.Series(kept_dates, name='date').to_csv(os.path.join(CONFIG['npy_folder'], 'dates.csv'), index=False, encoding='utf-8-sig')
    
    print(f"   ğŸ‰ æœ€ç»ˆçŸ©é˜µå½¢çŠ¶: {final_matrix.shape}")
    print(f"   ğŸ’¾ ç»“æœå·²ä¿å­˜è‡³: {CONFIG['npy_folder']}")

# ================= ğŸš€ ä¸»ç¨‹åºå…¥å£ =================
if __name__ == "__main__":
    # 1. æ”¶é›†è¯è¡¨
    vocab = module_1_collect_vocab()
    
    # 2. è®­ç»ƒæ˜ å°„ (æœ€æ ¸å¿ƒçš„ä¸€æ­¥)
    mapping, changes = module_2_build_mapping(vocab)
    
    # 3. å¯¼å‡ºExcelå¯¹æ¯”æ–‡ä»¶ & å¤„ç†æ•°æ®
    files = module_3_export_verification(mapping, changes)
    
    # 4. ç”Ÿæˆæœ€ç»ˆçŸ©é˜µ
    module_4_generate_matrix()