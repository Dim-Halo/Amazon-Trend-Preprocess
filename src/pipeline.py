import pandas as pd
import numpy as np
import os
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
from sentence_transformers import SentenceTransformer
import faiss
import gc
import json
import time

# ================= âš™ï¸ å…¨å±€é…ç½® (Config) =================
CONFIG = {
    'input_folder': './clean_data',          # è¾“å…¥ï¼šæ¸…æ´—åçš„æ•°æ®æ–‡ä»¶å¤¹
    'processed_folder': './processed_data',  # è¾“å‡ºï¼šåŠ äº†æ˜ å°„åˆ—çš„æ•°æ® (for model)
    'check_folder': './mapping_check',       # è¾“å‡ºï¼šäººå·¥æ£€æŸ¥ç”¨ Excel
    'npy_folder': './final_npy',             # è¾“å‡ºï¼šæœ€ç»ˆçŸ©é˜µ
    
    'term_col': 'normalized_term',           # æœç´¢è¯åˆ—å
    'date_col': 'æŠ¥å‘Šæ—¥æœŸ',                   # æ—¥æœŸåˆ—å
    'rank_col': 'æœç´¢é¢‘ç‡æ’å',               # æ’ååˆ—å
    
    'similarity_threshold': 0.725,            # ç›¸ä¼¼åº¦é˜ˆå€¼
    'device': 'cpu'                          # AMD 780M å¼ºåˆ¶ç”¨ CPU
}

# ç¡®ä¿æ–‡ä»¶å¤¹å­˜åœ¨
for folder in [CONFIG['processed_folder'], CONFIG['check_folder'], CONFIG['npy_folder']]:
    if not os.path.exists(folder):
        os.makedirs(folder)

# ================= ğŸ“¦ æ¨¡å— 1: å…¨å±€è¯æ±‡æ”¶é›† =================
def module_1_collect_vocab():
    """
    éå†æ‰€æœ‰æ–‡ä»¶ï¼Œæå–æ‰€æœ‰å”¯ä¸€çš„æœç´¢è¯ã€‚
    """
    print("\nğŸ“¦ [æ¨¡å— 1] å¯åŠ¨: å…¨å±€è¯æ±‡æ”¶é›†...")
    start_time = time.time()
    
    global_vocab = set()
    files = [f for f in os.listdir(CONFIG['input_folder']) if f.endswith('.csv')]
    
    for i, file in enumerate(files):
        path = os.path.join(CONFIG['input_folder'], file)
        try:
            # åªè¯»ä¸€åˆ—ï¼Œé€Ÿåº¦æå¿«
            df = pd.read_csv(path, usecols=[CONFIG['term_col']], encoding='utf-8-sig')
            terms = df[CONFIG['term_col']].dropna().unique().tolist()
            global_vocab.update(terms)
        except Exception as e:
            print(f"   âš ï¸ è·³è¿‡ {file}: {e}")
            
    vocab_list = sorted(list(global_vocab))
    print(f"   âœ… æ”¶é›†å®Œæˆ! å…¨ç½‘å”¯ä¸€è¯æ•°: {len(vocab_list)}")
    print(f"   â±ï¸ è€—æ—¶: {time.time() - start_time:.2f}s")
    return vocab_list

# ================= ğŸ§  æ¨¡å— 2: å‘é‡èšç±»ä¸æ˜ å°„ =================
def module_2_build_mapping(vocab_list):
    """
    å¯¹å”¯ä¸€è¯è¿›è¡Œå‘é‡åŒ–ï¼Œç”Ÿæˆæ˜ å°„å­—å…¸ã€‚
    """
    print("\nğŸ§  [æ¨¡å— 2] å¯åŠ¨: å‘é‡åŒ–ä¸èšç±» (AMD CPU Mode)...")
    start_time = time.time()
    
    # 1. åŠ è½½æ¨¡å‹
    model = SentenceTransformer('all-MiniLM-L6-v2', device=CONFIG['device'])
    
    # 2. å‘é‡åŒ–
    print(f"   âš¡ æ­£åœ¨è®¡ç®— {len(vocab_list)} ä¸ªè¯çš„å‘é‡...")
    embeddings = model.encode(vocab_list, batch_size=64, show_progress_bar=True, convert_to_numpy=True)
    faiss.normalize_L2(embeddings)
    
    # 3. FAISS èšç±»
    print("   ğŸ” æ­£åœ¨æœç´¢ç›¸ä¼¼è¯...")
    dim = embeddings.shape[1]
    index = faiss.IndexFlatIP(dim)
    index.add(embeddings)
    
    # èŒƒå›´æœç´¢
    limits, distances, indices = index.range_search(embeddings, CONFIG['similarity_threshold'])
    
    # 4. æ„å»ºå­—å…¸
    mapping_dict = {}
    change_log = [] # è®°å½•å“ªäº›è¯å‘ç”Ÿäº†å˜åŒ–ï¼Œæ–¹ä¾¿åç»­å¯¼å‡ºæ£€æŸ¥
    
    for i in range(len(vocab_list)):
        start, end = limits[i], limits[i+1]
        neighbor_indices = indices[start:end]
        
        # è·å–è¿™ä¸€ç»„çš„æ‰€æœ‰è¯
        neighbors = [vocab_list[idx] for idx in neighbor_indices]
        
        # ç­–ç•¥ï¼šé€‰æœ€çŸ­çš„è¯ä½œä¸ºæ ‡å‡†è¯ (Canonical Term)
        canonical = min(neighbors, key=len)
        
        current_word = vocab_list[i]
        mapping_dict[current_word] = canonical
        
        # å¦‚æœè¿™ä¸ªè¯è¢«æ”¹å˜äº†ï¼Œè®°å½•ä¸‹æ¥
        if current_word != canonical:
            change_log.append({
                'åŸå§‹è¯ (Original)': current_word,
                'æ˜ å°„å (Mapped)': canonical,
                'åŒç»„è¯æ•°': len(neighbors)
            })
            
    print(f"   âœ… æ˜ å°„æ„å»ºå®Œæˆ! å‘ç”Ÿæ˜ å°„çš„è¯å¯¹æ•°é‡: {len(change_log)}")
    print(f"   â±ï¸ è€—æ—¶: {time.time() - start_time:.2f}s")
    
    return mapping_dict, change_log

# ================= ğŸ“ æ¨¡å— 3: è¾“å‡ºéªŒè¯æ–‡ä»¶ (ä½ éœ€æ±‚çš„æ ¸å¿ƒ) =================
def module_3_export_verification(mapping_dict, change_log):
    """
    1. ç”Ÿæˆä¸€ä¸ªæ±‡æ€»çš„ Change Log Excel æ–¹ä¾¿äººå·¥å®¡æŸ¥ã€‚
    2. é‡æ–°å¤„ç† 52 ä¸ª CSVï¼Œå¢åŠ  'mapped_term' åˆ—å¹¶ä¿å­˜ã€‚
    """
    print("\nğŸ“ [æ¨¡å— 3] å¯åŠ¨: ç”ŸæˆéªŒè¯æ–‡ä»¶ä¸å¤„ç†æ•°æ®...")
    
    # --- ä»»åŠ¡ A: å¯¼å‡ºâ€œæ˜ å°„å…³ç³»è¡¨â€ (åªçœ‹å˜åŒ–çš„ï¼Œæ–¹ä¾¿ä½ å¿«é€Ÿæ£€æŸ¥) ---
    if change_log:
        df_log = pd.DataFrame(change_log)
        log_path = os.path.join(CONFIG['check_folder'], 'è¯ä¹‰æ˜ å°„å¯¹ç…§è¡¨_åªçœ‹å˜åŒ–.xlsx')
        # å­˜ä¸º Excelï¼Œæ–¹ä¾¿ä½ ç­›é€‰
        df_log.to_excel(log_path, index=False)
        print(f"   ğŸ‘ï¸ [äººå·¥æ£€æŸ¥] æ˜ å°„å¯¹ç…§è¡¨å·²ä¿å­˜è‡³: {log_path} (è¯·åŠ¡å¿…æ‰“å¼€çœ‹çœ‹!)")
    else:
        print("   âš ï¸ æ²¡æœ‰å‘ç°ä»»ä½•ç›¸ä¼¼è¯åˆå¹¶ï¼Œè¯·æ£€æŸ¥é˜ˆå€¼æ˜¯å¦å¤ªé«˜ã€‚")

    # --- ä»»åŠ¡ B: æ‰¹é‡å¤„ç† 52 ä¸ªæ–‡ä»¶ï¼Œå¢åŠ æ–°åˆ— ---
    print("   ğŸš€ æ­£åœ¨æ‰¹é‡å¤„ç†åŸå§‹æ–‡ä»¶ (å¢åŠ æ˜ å°„åˆ—)...")
    files = [f for f in os.listdir(CONFIG['input_folder']) if f.endswith('.csv')]
    
    processed_file_paths = []
    
    for file in files:
        in_path = os.path.join(CONFIG['input_folder'], file)
        out_path = os.path.join(CONFIG['processed_folder'], f"mapped_{file}")
        
        try:
            # è¯»å–
            df = pd.read_csv(in_path, encoding='utf-8-sig', parse_dates=[CONFIG['date_col']])
            
            # æ ¸å¿ƒæ“ä½œï¼šæ˜ å°„å¹¶æ–°å¢ä¸€åˆ—
            # mapä¸åˆ°çš„è¯ä¿æŒåŸæ · (fillna)
            df['mapped_term'] = df[CONFIG['term_col']].map(mapping_dict).fillna(df[CONFIG['term_col']])
            
            # ä¿å­˜
            df.to_csv(out_path, index=False, encoding='utf-8-sig')
            processed_file_paths.append(out_path)
            
        except Exception as e:
            print(f"   âŒ å¤„ç† {file} å¤±è´¥: {e}")
            
    print(f"   âœ… æ‰€æœ‰æ–‡ä»¶å·²å¤„ç†å¹¶ä¿å­˜è‡³: {CONFIG['processed_folder']}")
    return processed_file_paths

# ================= ğŸ“Š æ¨¡å— 4: ç”Ÿæˆ TimesNet çŸ©é˜µ =================
def module_4_generate_matrix():
    """
    ç›´æ¥æ‰«æ CONFIG['processed_folder'] æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰ CSV æ–‡ä»¶ã€‚
    æ‰§è¡Œ Pivot å’Œ Concatï¼Œç”Ÿæˆæœ€ç»ˆçŸ©é˜µã€‚
    """
    print("\nğŸ“Š [æ¨¡å— 4] å¯åŠ¨: æ‰«ææ–‡ä»¶å¤¹å¹¶ç”ŸæˆçŸ©é˜µ...")
    
    input_folder = CONFIG['processed_folder']
    
    # 1. æ£€æŸ¥æ–‡ä»¶å¤¹æ˜¯å¦å­˜åœ¨
    if not os.path.exists(input_folder):
        print(f"âŒ é”™è¯¯: è¾“å…¥æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {input_folder}")
        print("   è¯·å…ˆè¿è¡Œæ¨¡å— 3 ç”Ÿæˆæ•°æ®ï¼Œæˆ–æ£€æŸ¥è·¯å¾„é…ç½®ã€‚")
        return

    # 2. æ‰«ææ‰€æœ‰ CSV æ–‡ä»¶
    files = [f for f in os.listdir(input_folder) if f.endswith('.csv')]
    print(f"   ğŸ“‚ å‘ç° {len(files)} ä¸ª CSV æ–‡ä»¶ã€‚")
    
    if not files:
        print("âŒ é”™è¯¯: æ–‡ä»¶å¤¹æ˜¯ç©ºçš„ï¼Œæ²¡æœ‰ CSV æ–‡ä»¶ã€‚")
        return

    all_dfs = []
    
    # 3. å¾ªç¯å¤„ç†
    print("   ğŸš€ å¼€å§‹è¯»å–æ•°æ®...")
    for i, file in enumerate(files):
        file_path = os.path.join(input_folder, file)
        
        try:
            # è¯»å– (utf-8-sig å…¼å®¹æ€§æœ€ä½³)
            df = pd.read_csv(file_path, parse_dates=[CONFIG['date_col']], encoding='utf-8-sig')
            
            # æ¸…æ´—åˆ—å
            df.columns = df.columns.str.strip().str.replace('\ufeff', '')
            
            # æ’åè½¬æ•°å­— (å»é™¤é€—å·)
            if df[CONFIG['rank_col']].dtype == object:
                df[CONFIG['rank_col']] = df[CONFIG['rank_col']].astype(str).str.replace(',', '')
            df[CONFIG['rank_col']] = pd.to_numeric(df[CONFIG['rank_col']], errors='coerce')
            
            # èšåˆä¸é€è§†
            df_agg = df.groupby(['mapped_term', CONFIG['date_col']])[CONFIG['rank_col']].min().reset_index()
            df_pivot = df_agg.pivot(index='mapped_term', columns=CONFIG['date_col'], values=CONFIG['rank_col'])
            
            if not df_pivot.empty:
                all_dfs.append(df_pivot)
            
            # è¿›åº¦æ¡
            if (i + 1) % 10 == 0:
                print(f"      å·²å¤„ç† {i + 1}/{len(files)} ä¸ªæ–‡ä»¶...", end='\r')
                
        except Exception as e:
            print(f"   âš ï¸ è·³è¿‡æ–‡ä»¶ {file}: {e}")

    print(f"\n   âœ… æˆåŠŸè¯»å– {len(all_dfs)} ä¸ªæœ‰æ•ˆæ–‡ä»¶çš„æ•°æ®å—ã€‚")

    if not all_dfs:
        print("âŒ é”™è¯¯: æ‰€æœ‰æ–‡ä»¶å¤„ç†åå‡ä¸ºç©ºï¼æ— æ³•ç”ŸæˆçŸ©é˜µã€‚")
        return

    # 4. åˆå¹¶çŸ©é˜µ
    print("   ğŸ§© æ­£åœ¨æ‹¼æ¥å…¨é‡çŸ©é˜µ (Concat)...")
    final_df = pd.concat(all_dfs, axis=1)
    
    # 5. å¤„ç†é‡å¤åˆ— (Transpose Groupby ä¿®å¤ç‰ˆ)
    print("   ğŸ”„ å¤„ç†é‡å¤æ—¥æœŸåˆ— (Transpose Groupby)...")
    final_df = final_df.T.groupby(level=0).min().T
    
    # æŒ‰æ—¶é—´æ’åº
    final_df = final_df.sort_index(axis=1)
    
    # 6. æœ€ç»ˆæ¸…æ´—ä¸ä¿å­˜
    final_df.fillna(2000000, inplace=True) # å¡«å……ç©ºå€¼
    
    # è¿‡æ»¤é€»è¾‘ (ä¿ç•™å†å²æœ€é«˜æ’åå‰ 200ä¸‡ çš„è¯)
    valid_mask = final_df.min(axis=1) < 2000000
    
    final_matrix = np.log10(final_df.loc[valid_mask].values + 1)
    kept_terms = final_df.index[valid_mask]
    kept_dates = final_df.columns
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    if not os.path.exists(CONFIG['npy_folder']):
        os.makedirs(CONFIG['npy_folder'])

    # ä¿å­˜æ–‡ä»¶
    np.save(os.path.join(CONFIG['npy_folder'], 'timesnet_input.npy'), final_matrix)
    pd.Series(kept_terms, name='term').to_csv(os.path.join(CONFIG['npy_folder'], 'terms.csv'), index=False, encoding='utf-8-sig')
    pd.Series(kept_dates, name='date').to_csv(os.path.join(CONFIG['npy_folder'], 'dates.csv'), index=False, encoding='utf-8-sig')
    
    print(f"   ğŸ‰ æœ€ç»ˆçŸ©é˜µå½¢çŠ¶: {final_matrix.shape}")
    print(f"   ğŸ’¾ ç»“æœå·²ä¿å­˜è‡³: {CONFIG['npy_folder']}")

# ================= ğŸš€ ä¸»ç¨‹åºå…¥å£ =================
if __name__ == "__main__":
    # 1. æ”¶é›†è¯è¡¨
    vocab = module_1_collect_vocab()
    
    # 2. è®­ç»ƒæ˜ å°„
    mapping, changes = module_2_build_mapping(vocab)
    
    # 3. å¯¼å‡ºExcelå¯¹æ¯”æ–‡ä»¶ & å¤„ç†æ•°æ®
    files = module_3_export_verification(mapping, changes)
    
    # 4. ç”Ÿæˆæœ€ç»ˆçŸ©é˜µ
    module_4_generate_matrix()