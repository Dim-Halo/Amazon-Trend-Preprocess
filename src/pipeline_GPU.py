import pandas as pd
import numpy as np
import os
import gc
import json
import time
from sentence_transformers import SentenceTransformer
import faiss
import torch
from multiprocessing import Pool, cpu_count
from tqdm import tqdm
import psutil
import sys

# ================= âš™ï¸ å…¨å±€é…ç½® (Config) =================
CONFIG = {
    'input_folder': './clean_data',          # è¾“å…¥ï¼šæ¸…æ´—åçš„æ•°æ®æ–‡ä»¶å¤¹
    'processed_folder': './processed_data',  # è¾“å‡ºï¼šåŠ äº†æ˜ å°„åˆ—çš„æ•°æ® (for model)
    'check_folder': './mapping_check',       # è¾“å‡ºï¼šäººå·¥æ£€æŸ¥ç”¨ Excel
    'npy_folder': './final_npy',             # è¾“å‡ºï¼šæœ€ç»ˆçŸ©é˜µ
    
    'term_col': 'normalized_term',           # æœç´¢è¯åˆ—å
    'date_col': 'æŠ¥å‘Šæ—¥æœŸ',                   # æ—¥æœŸåˆ—å
    'rank_col': 'æœç´¢é¢‘ç‡æ’å',               # æ’ååˆ—å
    
    'similarity_threshold': 0.725,           # ç›¸ä¼¼åº¦é˜ˆå€¼
    
    # GPUé…ç½®
    'device': 'cuda:0' if torch.cuda.is_available() else 'cpu',  # è‡ªåŠ¨æ£€æµ‹GPU
    'faiss_gpu': True,                        # å¯ç”¨FAISS GPUåŠ é€Ÿ
    'batch_size': 512,                        # GPUæ‰¹æ¬¡å¤§å°
    'vectorization_batch': 512,               # å‘é‡åŒ–æ‰¹æ¬¡å¤§å°
    'parallel_process': True,                 # æ˜¯å¦å¹¶è¡Œå¤„ç†æ–‡ä»¶
    'max_workers': min(4, cpu_count())        # å¹¶è¡Œå·¥ä½œæ•°
}

# ç¡®ä¿æ–‡ä»¶å¤¹å­˜åœ¨
for folder in [CONFIG['processed_folder'], CONFIG['check_folder'], CONFIG['npy_folder']]:
    if not os.path.exists(folder):
        os.makedirs(folder)

# ================= ğŸ” GPUçŠ¶æ€æ£€æŸ¥ =================
def check_gpu_status():
    """æ£€æŸ¥GPUçŠ¶æ€"""
    print("\n" + "="*50)
    print("ğŸ” GPUçŠ¶æ€æ£€æŸ¥")
    print("="*50)
    
    try:
        import torch
        if torch.cuda.is_available():
            print(f"âœ… CUDAå¯ç”¨")
            print(f"   è®¾å¤‡æ•°é‡: {torch.cuda.device_count()}")
            
            for i in range(torch.cuda.device_count()):
                print(f"\n   GPU {i}: {torch.cuda.get_device_name(i)}")
                print(f"   æ˜¾å­˜æ€»é‡: {torch.cuda.get_device_properties(i).total_memory / 1e9:.2f} GB")
                print(f"   å½“å‰åˆ†é…: {torch.cuda.memory_allocated(i) / 1e9:.2f} GB")
                print(f"   ç¼“å­˜ä¿ç•™: {torch.cuda.memory_reserved(i) / 1e9:.2f} GB")
                
            # è®¾ç½®é»˜è®¤GPU
            torch.cuda.set_device(0)
            print(f"\nğŸ“Œ ä½¿ç”¨è®¾å¤‡: GPU 0 - {torch.cuda.get_device_name(0)}")
        else:
            print("âŒ CUDAä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨CPU")
            CONFIG['device'] = 'cpu'
            CONFIG['faiss_gpu'] = False
            print(f"ğŸ“Œ ä½¿ç”¨è®¾å¤‡: CPU")
            
        # æ£€æŸ¥FAISS GPUæ”¯æŒ
        try:
            import faiss
            if hasattr(faiss, 'StandardGpuResources'):
                print("\nâœ… FAISS GPUæ”¯æŒå¯ç”¨")
                if CONFIG['faiss_gpu'] and torch.cuda.is_available():
                    print("   ğŸš€ FAISS GPUåŠ é€Ÿå·²å¯ç”¨")
            else:
                print("\nâŒ FAISS GPUæ”¯æŒä¸å¯ç”¨")
                CONFIG['faiss_gpu'] = False
        except Exception as e:
            print(f"\nâš ï¸ FAISSå¯¼å…¥å¤±è´¥: {e}")
            CONFIG['faiss_gpu'] = False
            
    except Exception as e:
        print(f"âš ï¸ GPUæ£€æŸ¥å‡ºé”™: {e}")
        CONFIG['device'] = 'cpu'
        CONFIG['faiss_gpu'] = False
    
    print("="*50)
    
    # æ˜¾ç¤ºå†…å­˜çŠ¶æ€
    process = psutil.Process(os.getpid())
    print(f"ğŸ’¾ å½“å‰è¿›ç¨‹å†…å­˜ä½¿ç”¨: {process.memory_info().rss / 1024 / 1024:.1f} MB")
    
    if torch.cuda.is_available():
        print(f"ğŸ® GPUæ˜¾å­˜ä½¿ç”¨: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB / "
              f"{torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")
    
    print("="*50)
    return CONFIG['device']

# ================= ğŸ“¦ æ¨¡å— 1: å…¨å±€è¯æ±‡æ”¶é›† =================
def module_1_collect_vocab():
    """
    éå†æ‰€æœ‰æ–‡ä»¶ï¼Œæå–æ‰€æœ‰å”¯ä¸€çš„æœç´¢è¯ã€‚
    """
    print("\n" + "="*50)
    print("ğŸ“¦ [æ¨¡å— 1] å¯åŠ¨: å…¨å±€è¯æ±‡æ”¶é›†...")
    print("="*50)
    
    start_time = time.time()
    
    global_vocab = set()
    files = [f for f in os.listdir(CONFIG['input_folder']) if f.endswith('.csv')]
    
    print(f"ğŸ“‚ å‘ç° {len(files)} ä¸ªCSVæ–‡ä»¶")
    print("ğŸ” æ­£åœ¨æ”¶é›†è¯æ±‡...")
    
    for i, file in enumerate(tqdm(files, desc="æ”¶é›†è¯æ±‡")):
        path = os.path.join(CONFIG['input_folder'], file)
        try:
            # åªè¯»ä¸€åˆ—ï¼Œé€Ÿåº¦æå¿«
            df = pd.read_csv(path, usecols=[CONFIG['term_col']], encoding='utf-8-sig')
            terms = df[CONFIG['term_col']].dropna().unique().tolist()
            global_vocab.update(terms)
        except Exception as e:
            print(f"   âš ï¸ è·³è¿‡ {file}: {e}")
            
    vocab_list = sorted(list(global_vocab))
    
    # å†…å­˜æ¸…ç†
    del global_vocab
    gc.collect()
    
    print(f"\nâœ… æ”¶é›†å®Œæˆ!")
    print(f"   ğŸ“Š å…¨ç½‘å”¯ä¸€è¯æ•°: {len(vocab_list):,}")
    print(f"   ğŸ“ˆ è¯æ•°ç»Ÿè®¡:")
    print(f"      æ€»è¯æ•°: {len(vocab_list)}")
    
    # æ˜¾ç¤ºè¯é•¿ç»Ÿè®¡
    if vocab_list:
        avg_len = np.mean([len(str(word)) for word in vocab_list[:1000]])  # æŠ½æ ·ç»Ÿè®¡
        print(f"      å¹³å‡è¯é•¿: {avg_len:.1f} å­—ç¬¦")
    
    print(f"   â±ï¸ è€—æ—¶: {time.time() - start_time:.2f}ç§’")
    
    return vocab_list

# ================= ğŸ§  æ¨¡å— 2: GPUå‘é‡èšç±»ä¸æ˜ å°„ =================
def module_2_build_mapping(vocab_list):
    """
    ä½¿ç”¨ KNN + é˜ˆå€¼è¿‡æ»¤çš„ GPU å‘é‡èšç±»ä¸æ˜ å°„ï¼ˆç¨³å®šç‰ˆï¼‰
    """
    print("\n" + "="*50)
    print("ğŸ§  [æ¨¡å— 2] å¯åŠ¨: GPUå‘é‡åŒ–ä¸KNNèšç±»ï¼ˆç¨³å®šç‰ˆï¼‰")
    print("="*50)

    start_time = time.time()
    device = CONFIG['device']

    print(f"ğŸ“Š å¤„ç†è¯æ±‡é‡: {len(vocab_list):,}")
    print("ğŸš€ åŠ è½½ SentenceTransformer æ¨¡å‹...")
    model = SentenceTransformer('all-MiniLM-L6-v2', device=device)

    # ========= 1. å‘é‡åŒ– =========
    embeddings = model.encode(
        vocab_list,
        batch_size=CONFIG['vectorization_batch'],
        show_progress_bar=True,
        convert_to_numpy=True,
        normalize_embeddings=True
    )

    dim = embeddings.shape[1]
    print(f"âœ… å‘é‡åŒ–å®Œæˆ: {embeddings.shape}")

    # ========= 2. æ„å»º FAISS Index =========
    if CONFIG['faiss_gpu'] and 'cuda' in device:
        print("ğŸš€ ä½¿ç”¨ FAISS GPU Index (IndexFlatIP)")
        res = faiss.StandardGpuResources()
        res.setTempMemory(512 * 1024 * 1024)
        cpu_index = faiss.IndexFlatIP(dim)
        index = faiss.index_cpu_to_gpu(res, 0, cpu_index)
    else:
        print("ğŸ’» ä½¿ç”¨ FAISS CPU Index")
        index = faiss.IndexFlatIP(dim)

    index.add(embeddings)
    print(f"âœ… ç´¢å¼•æ„å»ºå®Œæˆï¼Œå‘é‡æ•°: {index.ntotal}")

    # ========= 3. KNN æœç´¢ =========
    K = 20  # â­ å¯è°ƒï¼š10~30 éƒ½å®‰å…¨
    print(f"ğŸ” æ‰§è¡Œ KNN æœç´¢ (K={K}) + ç›¸ä¼¼åº¦é˜ˆå€¼è¿‡æ»¤...")
    distances, indices = index.search(embeddings, K)

    # ========= 4. æ„å»ºæ˜ å°„ =========
    vocab_array = np.array(vocab_list, dtype=object)
    mapping_dict = {}
    change_log = []

    for i in tqdm(range(len(vocab_list)), desc="æ„å»ºæ˜ å°„"):
        sims = distances[i]
        nbrs = indices[i]

        # é˜ˆå€¼è¿‡æ»¤ï¼ˆä¿ç•™è‡ªå·±ï¼‰
        valid_mask = sims >= CONFIG['similarity_threshold']
        valid_indices = nbrs[valid_mask]

        if len(valid_indices) > 1:
            neighbors = vocab_array[valid_indices]

            # â­ æ–° canonical è§„åˆ™ï¼šè¯æ•°ä¼˜å…ˆï¼Œå…¶æ¬¡é•¿åº¦
            canonical = min(
                neighbors,
                key=lambda x: (len(x.split()), len(x))
            )

            mapping_dict[vocab_list[i]] = canonical

            if vocab_list[i] != canonical:
                change_log.append({
                    'åŸå§‹è¯ (Original)': vocab_list[i],
                    'æ˜ å°„å (Mapped)': canonical,
                    'åŒç»„è¯æ•°': len(neighbors),
                    'æœ€é«˜ç›¸ä¼¼åº¦': float(sims[valid_mask].max())
                })
        else:
            mapping_dict[vocab_list[i]] = vocab_list[i]

    print("\nâœ… æ˜ å°„æ„å»ºå®Œæˆ!")
    print(f"   ğŸ“Š æ€»è¯æ•°: {len(vocab_list):,}")
    print(f"   ğŸ”€ å‘ç”Ÿæ˜ å°„: {len(change_log):,}")
    print(f"   ğŸ“‰ æ˜ å°„æ¯”ä¾‹: {len(change_log)/len(vocab_list)*100:.2f}%")

    # ========= 5. æ¸…ç† =========
    del embeddings
    del index
    if 'res' in locals():
        del res
    del model
    torch.cuda.empty_cache()
    gc.collect()

    print(f"â±ï¸ æ¨¡å—è€—æ—¶: {time.time() - start_time:.2f} ç§’")
    return mapping_dict, change_log

# ================= ğŸ“ æ¨¡å— 3: è¾“å‡ºéªŒè¯æ–‡ä»¶ =================
def process_single_file(args):
    """å¤„ç†å•ä¸ªæ–‡ä»¶çš„å‡½æ•°ï¼Œç”¨äºå¹¶è¡Œå¤„ç†"""
    file, mapping_dict = args
    in_path = os.path.join(CONFIG['input_folder'], file)
    out_path = os.path.join(CONFIG['processed_folder'], f"mapped_{file}")
    
    try:
        # å°è¯•ä¸åŒç¼–ç è¯»å–
        encodings = ['utf-8-sig', 'utf-8', 'gbk', 'gb2312']
        df = None
        
        for enc in encodings:
            try:
                df = pd.read_csv(in_path, encoding=enc, parse_dates=[CONFIG['date_col']])
                break
            except:
                continue
        
        if df is None:
            print(f"   âŒ æ— æ³•è¯»å– {file}ï¼Œè·³è¿‡")
            return None
        
        # æ ¸å¿ƒæ“ä½œï¼šæ˜ å°„å¹¶æ–°å¢ä¸€åˆ—
        df['mapped_term'] = df[CONFIG['term_col']].map(mapping_dict).fillna(df[CONFIG['term_col']])
        
        # ä¿å­˜
        df.to_csv(out_path, index=False, encoding='utf-8-sig')
        return out_path
        
    except Exception as e:
        print(f"   âŒ å¤„ç† {file} å¤±è´¥: {e}")
        return None

def module_3_export_verification(mapping_dict, change_log):
    """
    1. ç”Ÿæˆä¸€ä¸ªæ±‡æ€»çš„ Change Log Excel æ–¹ä¾¿äººå·¥å®¡æŸ¥ã€‚
    2. é‡æ–°å¤„ç† 52 ä¸ª CSVï¼Œå¢åŠ  'mapped_term' åˆ—å¹¶ä¿å­˜ã€‚
    """
    print("\n" + "="*50)
    print("ğŸ“ [æ¨¡å— 3] å¯åŠ¨: ç”ŸæˆéªŒè¯æ–‡ä»¶ä¸å¤„ç†æ•°æ®...")
    print("="*50)
    
    start_time = time.time()
    
    # --- ä»»åŠ¡ A: å¯¼å‡º"æ˜ å°„å…³ç³»è¡¨" ---
    if change_log:
        print("ğŸ“Š ç”Ÿæˆæ˜ å°„å…³ç³»è¡¨...")
        df_log = pd.DataFrame(change_log)
        
        # æŒ‰åŒç»„è¯æ•°æ’åºï¼Œæ–¹ä¾¿æŸ¥çœ‹
        df_log = df_log.sort_values(['åŒç»„è¯æ•°', 'åŸå§‹è¯ (Original)'], ascending=[False, True])
        
        log_path = os.path.join(CONFIG['check_folder'], 'è¯ä¹‰æ˜ å°„å¯¹ç…§è¡¨_åªçœ‹å˜åŒ–.xlsx')
        
        # ä½¿ç”¨Excelå†™å…¥å™¨ï¼Œæ”¯æŒå¤§æ–‡ä»¶
        with pd.ExcelWriter(log_path, engine='openpyxl') as writer:
            df_log.to_excel(writer, index=False, sheet_name='æ˜ å°„å…³ç³»')
            
            # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
            stats_df = pd.DataFrame({
                'ç»Ÿè®¡é¡¹': ['æ€»è¯æ•°', 'å‘ç”Ÿæ˜ å°„è¯æ•°', 'æ˜ å°„æ¯”ä¾‹', 'å¹³å‡åŒç»„è¯æ•°', 'é˜ˆå€¼'],
                'æ•°å€¼': [
                    len(mapping_dict),
                    len(change_log),
                    f"{len(change_log)/len(mapping_dict)*100:.1f}%",
                    f"{df_log['åŒç»„è¯æ•°'].mean():.1f}",
                    CONFIG['similarity_threshold']
                ]
            })
            stats_df.to_excel(writer, index=False, sheet_name='ç»Ÿè®¡ä¿¡æ¯')
        
        print(f"âœ… [äººå·¥æ£€æŸ¥] æ˜ å°„å¯¹ç…§è¡¨å·²ä¿å­˜è‡³: {log_path}")
        print(f"   ğŸ“„ åŒ…å« {len(df_log)} æ¡æ˜ å°„è®°å½•")
    else:
        print("âš ï¸ æ²¡æœ‰å‘ç°ä»»ä½•ç›¸ä¼¼è¯åˆå¹¶ï¼Œè¯·æ£€æŸ¥é˜ˆå€¼æ˜¯å¦å¤ªé«˜ã€‚")
    
    # --- ä»»åŠ¡ B: æ‰¹é‡å¤„ç†æ–‡ä»¶ï¼Œå¢åŠ æ–°åˆ— ---
    print("\nğŸš€ æ‰¹é‡å¤„ç†åŸå§‹æ–‡ä»¶ (å¢åŠ æ˜ å°„åˆ—)...")
    files = [f for f in os.listdir(CONFIG['input_folder']) if f.endswith('.csv')]
    print(f"ğŸ“‚ å‘ç° {len(files)} ä¸ªæ–‡ä»¶éœ€è¦å¤„ç†")
    
    processed_file_paths = []
    
    if CONFIG['parallel_process'] and len(files) > 5:
        print(f"âš¡ å¯ç”¨å¹¶è¡Œå¤„ç†ï¼Œä½¿ç”¨ {CONFIG['max_workers']} ä¸ªè¿›ç¨‹...")
        
        # å‡†å¤‡å‚æ•°
        args_list = [(file, mapping_dict) for file in files]
        
        # ä½¿ç”¨è¿›ç¨‹æ± å¹¶è¡Œå¤„ç†
        with Pool(processes=CONFIG['max_workers']) as pool:
            results = list(tqdm(pool.imap(process_single_file, args_list), 
                              total=len(files), 
                              desc="å¹¶è¡Œå¤„ç†æ–‡ä»¶"))
        
        processed_file_paths = [r for r in results if r is not None]
        
    else:
        print("ğŸ’» ä½¿ç”¨ä¸²è¡Œå¤„ç†...")
        for file in tqdm(files, desc="å¤„ç†æ–‡ä»¶"):
            args = (file, mapping_dict)
            result = process_single_file(args)
            if result:
                processed_file_paths.append(result)
    
    print(f"\nâœ… æ–‡ä»¶å¤„ç†å®Œæˆ!")
    print(f"   ğŸ“ æˆåŠŸå¤„ç†: {len(processed_file_paths)}/{len(files)} ä¸ªæ–‡ä»¶")
    print(f"   ğŸ“‚ ä¿å­˜è‡³: {CONFIG['processed_folder']}")
    
    # ä¿å­˜æ˜ å°„å­—å…¸ä¸ºJSONï¼Œæ–¹ä¾¿åç»­ä½¿ç”¨
    print("ğŸ’¾ ä¿å­˜æ˜ å°„å­—å…¸...")
    dict_path = os.path.join(CONFIG['check_folder'], 'mapping_dictionary.json')
    
    # åªä¿å­˜éƒ¨åˆ†æ˜ å°„ï¼ˆå‰1000æ¡ï¼‰ä½œä¸ºç¤ºä¾‹
    sample_dict = dict(list(mapping_dict.items())[:1000])
    with open(dict_path, 'w', encoding='utf-8') as f:
        json.dump(sample_dict, f, ensure_ascii=False, indent=2)
    
    print(f"   ğŸ“„ æ˜ å°„å­—å…¸ç¤ºä¾‹å·²ä¿å­˜: {dict_path}")
    print(f"â±ï¸ è€—æ—¶: {time.time() - start_time:.2f}ç§’")
    
    return processed_file_paths

# ================= ğŸ“Š æ¨¡å— 4: ç”Ÿæˆ TimesNet çŸ©é˜µ =================
def module_4_generate_matrix():
    """
    ç›´æ¥æ‰«æ CONFIG['processed_folder'] æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰ CSV æ–‡ä»¶ã€‚
    æ‰§è¡Œ Pivot å’Œ Concatï¼Œç”Ÿæˆæœ€ç»ˆçŸ©é˜µã€‚
    """
    print("\n" + "="*50)
    print("ğŸ“Š [æ¨¡å— 4] å¯åŠ¨: æ‰«ææ–‡ä»¶å¤¹å¹¶ç”ŸæˆçŸ©é˜µ...")
    print("="*50)
    
    start_time = time.time()
    input_folder = CONFIG['processed_folder']
    
    # 1. æ£€æŸ¥æ–‡ä»¶å¤¹æ˜¯å¦å­˜åœ¨
    if not os.path.exists(input_folder):
        print(f"âŒ é”™è¯¯: è¾“å…¥æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {input_folder}")
        print("   è¯·å…ˆè¿è¡Œæ¨¡å— 3 ç”Ÿæˆæ•°æ®ï¼Œæˆ–æ£€æŸ¥è·¯å¾„é…ç½®ã€‚")
        return
    
    # 2. æ‰«ææ‰€æœ‰ CSV æ–‡ä»¶
    files = [f for f in os.listdir(input_folder) if f.endswith('.csv')]
    print(f"ğŸ“‚ å‘ç° {len(files)} ä¸ª CSV æ–‡ä»¶")
    
    if not files:
        print("âŒ é”™è¯¯: æ–‡ä»¶å¤¹æ˜¯ç©ºçš„ï¼Œæ²¡æœ‰ CSV æ–‡ä»¶ã€‚")
        return
    
    all_dfs = []
    skipped_files = []
    
    # 3. å¾ªç¯å¤„ç†
    print("ğŸš€ å¼€å§‹è¯»å–å’Œå¤„ç†æ•°æ®...")
    
    for i, file in enumerate(tqdm(files, desc="å¤„ç†æ–‡ä»¶")):
        file_path = os.path.join(input_folder, file)
        
        try:
            # è¯»å–æ–‡ä»¶
            df = pd.read_csv(file_path, parse_dates=[CONFIG['date_col']], encoding='utf-8-sig')
            
            # æ¸…æ´—åˆ—å
            df.columns = df.columns.str.strip().str.replace('\ufeff', '')
            
            # æ£€æŸ¥å¿…è¦åˆ—æ˜¯å¦å­˜åœ¨
            required_cols = ['mapped_term', CONFIG['date_col'], CONFIG['rank_col']]
            missing_cols = [col for col in required_cols if col not in df.columns]
            
            if missing_cols:
                print(f"   âš ï¸ æ–‡ä»¶ {file} ç¼ºå°‘åˆ—: {missing_cols}ï¼Œè·³è¿‡")
                skipped_files.append((file, f"ç¼ºå°‘åˆ—: {missing_cols}"))
                continue
            
            # æ’åè½¬æ•°å­— (å»é™¤é€—å·)
            if df[CONFIG['rank_col']].dtype == object:
                df[CONFIG['rank_col']] = df[CONFIG['rank_col']].astype(str).str.replace(',', '')
            
            df[CONFIG['rank_col']] = pd.to_numeric(df[CONFIG['rank_col']], errors='coerce')
            
            # å»é™¤ç©ºå€¼
            df = df.dropna(subset=['mapped_term', CONFIG['date_col'], CONFIG['rank_col']])
            
            if len(df) == 0:
                print(f"   âš ï¸ æ–‡ä»¶ {file} æ²¡æœ‰æœ‰æ•ˆæ•°æ®ï¼Œè·³è¿‡")
                skipped_files.append((file, "æ²¡æœ‰æœ‰æ•ˆæ•°æ®"))
                continue
            
            # èšåˆä¸é€è§†
            df_agg = df.groupby(['mapped_term', CONFIG['date_col']])[CONFIG['rank_col']].min().reset_index()
            df_pivot = df_agg.pivot(index='mapped_term', columns=CONFIG['date_col'], values=CONFIG['rank_col'])
            
            if not df_pivot.empty:
                all_dfs.append(df_pivot)
            else:
                skipped_files.append((file, "é€è§†åä¸ºç©º"))
                
        except Exception as e:
            error_msg = str(e)[:100]  # æˆªå–é”™è¯¯ä¿¡æ¯å‰100å­—ç¬¦
            print(f"   âš ï¸ è·³è¿‡æ–‡ä»¶ {file}: {error_msg}")
            skipped_files.append((file, error_msg))
    
    print(f"\nâœ… æ•°æ®å¤„ç†å®Œæˆ!")
    print(f"   ğŸ“Š æˆåŠŸè¯»å– {len(all_dfs)}/{len(files)} ä¸ªæ–‡ä»¶çš„æ•°æ®å—")
    
    if skipped_files:
        print(f"   âš ï¸ è·³è¿‡äº† {len(skipped_files)} ä¸ªæ–‡ä»¶")
        # ä¿å­˜è·³è¿‡çš„æ–‡ä»¶åˆ—è¡¨
        skipped_df = pd.DataFrame(skipped_files, columns=['æ–‡ä»¶å', 'åŸå› '])
        skipped_path = os.path.join(CONFIG['check_folder'], 'skipped_files.csv')
        skipped_df.to_csv(skipped_path, index=False, encoding='utf-8-sig')
        print(f"   ğŸ“„ è·³è¿‡çš„æ–‡ä»¶åˆ—è¡¨å·²ä¿å­˜: {skipped_path}")
    
    if not all_dfs:
        print("âŒ é”™è¯¯: æ‰€æœ‰æ–‡ä»¶å¤„ç†åå‡ä¸ºç©ºï¼æ— æ³•ç”ŸæˆçŸ©é˜µã€‚")
        return
    
    # 4. åˆå¹¶çŸ©é˜µ
    print("ğŸ§© æ­£åœ¨æ‹¼æ¥å…¨é‡çŸ©é˜µ...")
    merge_start = time.time()
    
    # åˆ†æ‰¹åˆå¹¶ï¼Œé¿å…å†…å­˜æº¢å‡º
    batch_size = 10
    merged_dfs = []
    
    for i in range(0, len(all_dfs), batch_size):
        batch = all_dfs[i:i+batch_size]
        batch_df = pd.concat(batch, axis=1)
        
        # å¤„ç†é‡å¤åˆ—
        batch_df = batch_df.T.groupby(level=0).min().T
        merged_dfs.append(batch_df)
        
        # æ¸…ç†å†…å­˜
        del batch, batch_df
        gc.collect()
    
    # æœ€ç»ˆåˆå¹¶
    if len(merged_dfs) > 1:
        final_df = pd.concat(merged_dfs, axis=1)
        final_df = final_df.T.groupby(level=0).min().T
    else:
        final_df = merged_dfs[0]
    
    print(f"   âœ… çŸ©é˜µåˆå¹¶å®Œæˆï¼Œè€—æ—¶: {time.time() - merge_start:.2f}ç§’")
    
    # 5. æŒ‰æ—¶é—´æ’åº
    final_df = final_df.sort_index(axis=1)
    
    # 6. æœ€ç»ˆæ¸…æ´—ä¸ä¿å­˜
    print("ğŸ§¹ æœ€ç»ˆæ¸…æ´—å’Œä¿å­˜...")
    
    # å¡«å……ç©ºå€¼
    final_df.fillna(2000000, inplace=True)
    
    # è¿‡æ»¤é€»è¾‘ (ä¿ç•™å†å²æœ€é«˜æ’åå‰ 200ä¸‡ çš„è¯)
    valid_mask = final_df.min(axis=1) < 2000000
    final_filtered = final_df.loc[valid_mask]
    
    print(f"   ğŸ“Š è¿‡æ»¤å‰: {final_df.shape[0]:,} ä¸ªè¯")
    print(f"   ğŸ“Š è¿‡æ»¤å: {final_filtered.shape[0]:,} ä¸ªè¯")
    print(f"   ğŸ“Š è¿‡æ»¤æ¯”ä¾‹: {(1 - final_filtered.shape[0]/final_df.shape[0])*100:.1f}%")
    
    # è½¬æ¢ä¸ºlog10çŸ©é˜µ
    final_matrix = np.log10(final_filtered.values + 1)
    kept_terms = final_filtered.index
    kept_dates = final_filtered.columns
    
    # 7. ä¿å­˜æ–‡ä»¶
    print("ğŸ’¾ ä¿å­˜ç»“æœæ–‡ä»¶...")
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    if not os.path.exists(CONFIG['npy_folder']):
        os.makedirs(CONFIG['npy_folder'])
    
    # ä¿å­˜npyæ–‡ä»¶
    npy_path = os.path.join(CONFIG['npy_folder'], 'timesnet_input.npy')
    np.save(npy_path, final_matrix)
    print(f"   âœ… çŸ©é˜µæ–‡ä»¶: {npy_path}")
    print(f"      å½¢çŠ¶: {final_matrix.shape}")
    print(f"      å¤§å°: {final_matrix.nbytes / 1024 / 1024:.1f} MB")
    
    # ä¿å­˜æœ¯è¯­æ–‡ä»¶
    terms_path = os.path.join(CONFIG['npy_folder'], 'terms.csv')
    pd.Series(kept_terms, name='term').to_csv(terms_path, index=False, encoding='utf-8-sig')
    print(f"   âœ… æœ¯è¯­æ–‡ä»¶: {terms_path}")
    print(f"      æœ¯è¯­æ•°é‡: {len(kept_terms):,}")
    
    # ä¿å­˜æ—¥æœŸæ–‡ä»¶
    dates_path = os.path.join(CONFIG['npy_folder'], 'dates.csv')
    pd.Series(kept_dates, name='date').to_csv(dates_path, index=False, encoding='utf-8-sig')
    print(f"   âœ… æ—¥æœŸæ–‡ä»¶: {dates_path}")
    print(f"      æ—¥æœŸæ•°é‡: {len(kept_dates)}")
    print(f"      æ—¥æœŸèŒƒå›´: {kept_dates.min()} åˆ° {kept_dates.max()}")
    
    # ä¿å­˜çŸ©é˜µç»Ÿè®¡ä¿¡æ¯
    stats = {
        'matrix_shape': final_matrix.shape,
        'min_value': float(final_matrix.min()),
        'max_value': float(final_matrix.max()),
        'mean_value': float(final_matrix.mean()),
        'std_value': float(final_matrix.std()),
        'n_terms': len(kept_terms),
        'n_dates': len(kept_dates),
        'date_range': [str(kept_dates.min()), str(kept_dates.max())],
        'generation_time': time.strftime('%Y-%m-%d %H:%M:%S'),
        'processing_time_seconds': time.time() - start_time
    }
    
    stats_path = os.path.join(CONFIG['npy_folder'], 'matrix_statistics.json')
    with open(stats_path, 'w', encoding='utf-8') as f:
        json.dump(stats, f, ensure_ascii=False, indent=2)
    
    print(f"   âœ… ç»Ÿè®¡ä¿¡æ¯: {stats_path}")
    
    print(f"\nğŸ‰ æœ€ç»ˆçŸ©é˜µç”Ÿæˆå®Œæˆ!")
    print(f"â±ï¸ æ€»è€—æ—¶: {time.time() - start_time:.2f}ç§’")

# ================= ğŸ¯ æ¨¡å— 5: è¾“å‡ºéªŒè¯ =================
def verify_output():
    """éªŒè¯è¾“å‡ºæ–‡ä»¶çš„å®Œæ•´æ€§"""
    print("\n" + "="*50)
    print("ğŸ” è¾“å‡ºéªŒè¯...")
    print("="*50)
    
    verification_passed = True
    
    # æ£€æŸ¥npyæ–‡ä»¶
    npy_path = os.path.join(CONFIG['npy_folder'], 'timesnet_input.npy')
    if os.path.exists(npy_path):
        try:
            matrix = np.load(npy_path)
            print(f"âœ… çŸ©é˜µæ–‡ä»¶: {matrix.shape}")
            print(f"   æœ€å°å€¼: {matrix.min():.4f}")
            print(f"   æœ€å¤§å€¼: {matrix.max():.4f}")
            print(f"   å¹³å‡å€¼: {matrix.mean():.4f}")
            print(f"   æ ‡å‡†å·®: {matrix.std():.4f}")
            
            # æ£€æŸ¥NaNå€¼
            nan_count = np.isnan(matrix).sum()
            if nan_count > 0:
                print(f"âš ï¸ è­¦å‘Š: çŸ©é˜µä¸­åŒ…å« {nan_count} ä¸ªNaNå€¼")
                verification_passed = False
            else:
                print(f"âœ… çŸ©é˜µæ— NaNå€¼")
                
        except Exception as e:
            print(f"âŒ æ— æ³•åŠ è½½çŸ©é˜µæ–‡ä»¶: {e}")
            verification_passed = False
    else:
        print("âŒ çŸ©é˜µæ–‡ä»¶ä¸å­˜åœ¨")
        verification_passed = False
    
    # æ£€æŸ¥æœ¯è¯­æ–‡ä»¶
    terms_path = os.path.join(CONFIG['npy_folder'], 'terms.csv')
    if os.path.exists(terms_path):
        try:
            terms_df = pd.read_csv(terms_path, encoding='utf-8-sig')
            print(f"âœ… æœ¯è¯­æ–‡ä»¶: {len(terms_df)} ä¸ªæœ¯è¯­")
            
            # æ£€æŸ¥é‡å¤é¡¹
            duplicates = terms_df['term'].duplicated().sum()
            if duplicates > 0:
                print(f"âš ï¸ è­¦å‘Š: æœ¯è¯­æ–‡ä»¶åŒ…å« {duplicates} ä¸ªé‡å¤é¡¹")
                verification_passed = False
            else:
                print(f"âœ… æœ¯è¯­æ— é‡å¤")
                
        except Exception as e:
            print(f"âŒ æ— æ³•åŠ è½½æœ¯è¯­æ–‡ä»¶: {e}")
            verification_passed = False
    else:
        print("âŒ æœ¯è¯­æ–‡ä»¶ä¸å­˜åœ¨")
        verification_passed = False
    
    # æ£€æŸ¥æ—¥æœŸæ–‡ä»¶
    dates_path = os.path.join(CONFIG['npy_folder'], 'dates.csv')
    if os.path.exists(dates_path):
        try:
            dates_df = pd.read_csv(dates_path, encoding='utf-8-sig')
            print(f"âœ… æ—¥æœŸæ–‡ä»¶: {len(dates_df)} ä¸ªæ—¥æœŸ")
            print(f"   æ—¥æœŸèŒƒå›´: {dates_df['date'].min()} åˆ° {dates_df['date'].max()}")
        except Exception as e:
            print(f"âŒ æ— æ³•åŠ è½½æ—¥æœŸæ–‡ä»¶: {e}")
            verification_passed = False
    else:
        print("âŒ æ—¥æœŸæ–‡ä»¶ä¸å­˜åœ¨")
        verification_passed = False
    
    # æ£€æŸ¥æ–‡ä»¶å¤§å°ä¸€è‡´æ€§
    if os.path.exists(npy_path) and os.path.exists(terms_path):
        matrix = np.load(npy_path)
        terms_df = pd.read_csv(terms_path, encoding='utf-8-sig')
        
        if matrix.shape[0] != len(terms_df):
            print(f"âŒ é”™è¯¯: çŸ©é˜µè¡Œæ•° ({matrix.shape[0]}) ä¸æœ¯è¯­æ•°é‡ ({len(terms_df)}) ä¸åŒ¹é…")
            verification_passed = False
        else:
            print(f"âœ… çŸ©é˜µä¸æœ¯è¯­æ•°é‡åŒ¹é…: {matrix.shape[0]}")
    
    print("="*50)
    if verification_passed:
        print("ğŸ‰ æ‰€æœ‰éªŒè¯é€šè¿‡!")
    else:
        print("âš ï¸ éªŒè¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥è¾“å‡ºæ–‡ä»¶")
    
    return verification_passed

# ================= ğŸš€ ä¸»ç¨‹åºå…¥å£ =================
if __name__ == "__main__":
    print("\n" + "="*70)
    print("ğŸš€ è¯­ä¹‰èšç±»ä¸çŸ©é˜µç”Ÿæˆç³»ç»Ÿ - GPUåŠ é€Ÿç‰ˆ")
    print("="*70)
    
    # è®°å½•æ€»å¼€å§‹æ—¶é—´
    total_start_time = time.time()
    
    try:
        # æ­¥éª¤ 0: æ£€æŸ¥GPUçŠ¶æ€
        device = check_gpu_status()
        
        # æ­¥éª¤ 1: æ”¶é›†è¯è¡¨
        vocab = module_1_collect_vocab()
        
        if not vocab:
            print("âŒ é”™è¯¯: æ²¡æœ‰æ”¶é›†åˆ°è¯æ±‡ï¼Œç¨‹åºç»ˆæ­¢")
            sys.exit(1)
        
        # æ­¥éª¤ 2: è®­ç»ƒæ˜ å°„ (GPUåŠ é€Ÿ)
        mapping, changes = module_2_build_mapping(vocab)
        
        # æ¸…ç†è¯æ±‡è¡¨å†…å­˜
        del vocab
        gc.collect()
        
        # æ­¥éª¤ 3: å¯¼å‡ºExcelå¯¹æ¯”æ–‡ä»¶ & å¤„ç†æ•°æ®
        files = module_3_export_verification(mapping, changes)
        
        # æ¸…ç†æ˜ å°„å­—å…¸å†…å­˜
        del mapping
        gc.collect()
        
        # æ­¥éª¤ 4: ç”Ÿæˆæœ€ç»ˆçŸ©é˜µ
        module_4_generate_matrix()
        
        # æ­¥éª¤ 5: éªŒè¯è¾“å‡º
        verification_result = verify_output()
        
        # æ€»è€—æ—¶ç»Ÿè®¡
        total_time = time.time() - total_start_time
        hours, remainder = divmod(total_time, 3600)
        minutes, seconds = divmod(remainder, 60)
        
        print("\n" + "="*70)
        print("ğŸ‰ æ‰€æœ‰æµç¨‹å®Œæˆ!")
        print("="*70)
        print(f"â±ï¸ æ€»è¿è¡Œæ—¶é—´: {int(hours)}å°æ—¶ {int(minutes)}åˆ†é’Ÿ {seconds:.1f}ç§’")
        
        if verification_result:
            print("âœ… è¾“å‡ºéªŒè¯é€šè¿‡ï¼Œå¯ä»¥ç”¨äºTimesNetè®­ç»ƒ")
        else:
            print("âš ï¸ è¾“å‡ºéªŒè¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥è¾“å‡ºæ–‡ä»¶")
            
        print(f"\nğŸ“ è¾“å‡ºæ–‡ä»¶ä½ç½®:")
        print(f"   1. å¤„ç†åçš„æ•°æ®: {CONFIG['processed_folder']}")
        print(f"   2. æ£€æŸ¥æ–‡ä»¶: {CONFIG['check_folder']}")
        print(f"   3. æœ€ç»ˆçŸ©é˜µ: {CONFIG['npy_folder']}")
        print("="*70)
        
    except KeyboardInterrupt:
        print("\n\nâš ï¸ ç”¨æˆ·ä¸­æ–­ï¼Œç¨‹åºç»ˆæ­¢")
        sys.exit(0)
        
    except Exception as e:
        print(f"\nâŒ ç¨‹åºè¿è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)